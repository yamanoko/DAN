{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from basic.transforms import aug_config\n",
    "from OCR.ocr_dataset_manager import OCRDataset, OCRDatasetManager\n",
    "from OCR.document_OCR.dan.trainer_dan import Manager\n",
    "from OCR.document_OCR.dan.models_dan import GlobalHTADecoder\n",
    "from basic.models import FCN_Encoder\n",
    "from basic.scheduler import exponential_dropout_scheduler, linear_scheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "dataset_name = \"READ_2016\"  # [\"RIMES\", \"READ_2016\"]\n",
    "dataset_level = \"page\"  # [\"page\", \"double_page\"]\n",
    "dataset_variant = \"_sem\"\n",
    "\n",
    "# max number of lines for synthetic documents\n",
    "max_nb_lines = {\n",
    "\t\"RIMES\": 40,\n",
    "\t\"READ_2016\": 30,\n",
    "}\n",
    "\n",
    "dataset_params = {\n",
    "\t\t\t\"dataset_manager\": OCRDatasetManager,\n",
    "\t\t\t\"dataset_class\": OCRDataset,\n",
    "\t\t\t\"use_ddp\": False,\n",
    "\t\t\t\"batch_size\": 1,\n",
    "\t\t\t\"valid_batch_size\": 4,\n",
    "\t\t\t\"test_batch_size\": 4,\n",
    "\t\t\t\"num_gpu\": torch.cuda.device_count(),\n",
    "\t\t\t\"worker_per_gpu\": 4,\n",
    "\t\t\t\"datasets\": {\n",
    "\t\t\t\tdataset_name: \"/home/yukinori/Desktop/RenAIssance2025/DAN/Datasets/formatted/{}_{}{}\".format(dataset_name, dataset_level, dataset_variant),\n",
    "\t\t\t},\n",
    "\t\t\t\"train\": {\n",
    "\t\t\t\t\"name\": \"{}-train\".format(dataset_name),\n",
    "\t\t\t\t\"datasets\": [(dataset_name, \"train\"), ],\n",
    "\t\t\t},\n",
    "\t\t\t\"valid\": {\n",
    "\t\t\t\t\"{}-valid\".format(dataset_name): [(dataset_name, \"valid\"), ],\n",
    "\t\t\t},\n",
    "\t\t\t\"config\": {\n",
    "\t\t\t\t\"load_in_memory\": True,  # Load all images in CPU memory\n",
    "\t\t\t\t\"worker_per_gpu\": 4,  # Num of parallel processes per gpu for data loading\n",
    "\t\t\t\t\"width_divisor\": 8,  # Image width will be divided by 8\n",
    "\t\t\t\t\"height_divisor\": 32,  # Image height will be divided by 32\n",
    "\t\t\t\t\"padding_value\": 0,  # Image padding value\n",
    "\t\t\t\t\"padding_token\": None,  # Label padding value\n",
    "\t\t\t\t\"charset_mode\": \"seq2seq\",  # add end-of-transcription ans start-of-transcription tokens to charset\n",
    "\t\t\t\t\"constraints\": [\"add_eot\", \"add_sot\"],  # add end-of-transcription ans start-of-transcription tokens in labels\n",
    "\t\t\t\t\"normalize\": False,  # Normalize with mean and variance of training dataset\n",
    "\t\t\t\t\"preprocessings\": [\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"type\": \"to_RGB\",\n",
    "\t\t\t\t\t\t# if grayscaled image, produce RGB one (3 channels with same value) otherwise do nothing\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"augmentation\": aug_config(0.9, 0.1),\n",
    "\t\t\t\t# \"synthetic_data\": None,\n",
    "\t\t\t\t\"synthetic_data\": {\n",
    "\t\t\t\t\t\"init_proba\": 0.9,  # begin proba to generate synthetic document\n",
    "\t\t\t\t\t\"end_proba\": 0.2,  # end proba to generate synthetic document\n",
    "\t\t\t\t\t\"num_steps_proba\": 200000,  # linearly decrease the percent of synthetic document from 90% to 20% through 200000 samples\n",
    "\t\t\t\t\t\"proba_scheduler_function\": linear_scheduler,  # decrease proba rate linearly\n",
    "\t\t\t\t\t\"start_scheduler_at_max_line\": True,  # start decreasing proba only after curriculum reach max number of lines\n",
    "\t\t\t\t\t\"dataset_level\": dataset_level,\n",
    "\t\t\t\t\t\"curriculum\": True,  # use curriculum learning (slowly increase number of lines per synthetic samples)\n",
    "\t\t\t\t\t\"crop_curriculum\": True,  # during curriculum learning, crop images under the last text line\n",
    "\t\t\t\t\t\"curr_start\": 0,  # start curriculum at iteration\n",
    "\t\t\t\t\t\"curr_step\": 10000,  # interval to increase the number of lines for curriculum learning\n",
    "\t\t\t\t\t\"min_nb_lines\": 1,  # initial number of lines for curriculum learning\n",
    "\t\t\t\t\t\"max_nb_lines\": max_nb_lines[dataset_name],  # maximum number of lines for curriculum learning\n",
    "\t\t\t\t\t\"padding_value\": 255,\n",
    "\t\t\t\t\t# config for synthetic line generation\n",
    "\t\t\t\t\t\"config\": {\n",
    "\t\t\t\t\t\t\"background_color_default\": (255, 255, 255),\n",
    "\t\t\t\t\t\t\"background_color_eps\": 15,\n",
    "\t\t\t\t\t\t\"text_color_default\": (0, 0, 0),\n",
    "\t\t\t\t\t\t\"text_color_eps\": 15,\n",
    "\t\t\t\t\t\t\"font_size_min\": 35,\n",
    "\t\t\t\t\t\t\"font_size_max\": 45,\n",
    "\t\t\t\t\t\t\"color_mode\": \"RGB\",\n",
    "\t\t\t\t\t\t\"padding_left_ratio_min\": 0.00,\n",
    "\t\t\t\t\t\t\"padding_left_ratio_max\": 0.05,\n",
    "\t\t\t\t\t\t\"padding_right_ratio_min\": 0.02,\n",
    "\t\t\t\t\t\t\"padding_right_ratio_max\": 0.2,\n",
    "\t\t\t\t\t\t\"padding_top_ratio_min\": 0.02,\n",
    "\t\t\t\t\t\t\"padding_top_ratio_max\": 0.1,\n",
    "\t\t\t\t\t\t\"padding_bottom_ratio_min\": 0.02,\n",
    "\t\t\t\t\t\t\"padding_bottom_ratio_max\": 0.1,\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "my_dataset = dataset_params[\"dataset_manager\"](dataset_params)\n",
    "my_dataset.load_datasets()\n",
    "my_dataset.load_ddp_samplers()\n",
    "my_dataset.load_dataloaders()\n",
    "my_dataset.train_dataset.training_info = {\n",
    "\t\"epoch\": 0,\n",
    "\t\"step\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = next(iter(my_dataset.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'names': ['synthetic_data_0'],\n",
       " 'ids': [0],\n",
       " 'nb_lines': [1],\n",
       " 'nb_cols': [1],\n",
       " 'labels': tensor([[100,  97,  96,  10,  12,  91,  92,  99]]),\n",
       " 'reverse_labels': tensor([[100,  92,  91,  12,  10,  96,  97,  99]]),\n",
       " 'raw_labels': ['ⓟⓝ13ⓃⓅ'],\n",
       " 'unchanged_labels': ['ⓟⓝ13ⓃⓅ'],\n",
       " 'labels_len': [7],\n",
       " 'imgs': tensor([[[[120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           ...,\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.]],\n",
       " \n",
       "          [[120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           ...,\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.]],\n",
       " \n",
       "          [[120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           ...,\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.],\n",
       "           [120., 120., 120.,  ..., 120., 120., 120.]]]]),\n",
       " 'imgs_shape': [[57, 1221, 3]],\n",
       " 'imgs_reduced_shape': [[2, 153, 3]],\n",
       " 'imgs_position': [[[0, 57], [0, 1221]]],\n",
       " 'imgs_reduced_position': [[array([0, 2]), array([  0, 153])]],\n",
       " 'line_raw': [['ⓟⓝ13ⓃⓅ']],\n",
       " 'line_labels': [tensor([[97, 96, 10, 12, 91, 92]])],\n",
       " 'line_labels_len': [[6]],\n",
       " 'nb_words': [1],\n",
       " 'word_raw': [['ⓟⓝ13ⓃⓅ']],\n",
       " 'word_labels': [tensor([[97, 96, 10, 12, 91, 92]])],\n",
       " 'word_labels_len': [[6]],\n",
       " 'applied_da': [['Sharpen', 'Erosion']]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 57, 1221])\n"
     ]
    }
   ],
   "source": [
    "print(sample_data[\"imgs\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x74096949bb50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(sample_data[\"imgs\"][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '(',\n",
       " ')',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '¬',\n",
       " '¾',\n",
       " 'Ö',\n",
       " 'ß',\n",
       " 'ä',\n",
       " 'ö',\n",
       " 'ü',\n",
       " 'ÿ',\n",
       " 'ā',\n",
       " 'ē',\n",
       " 'ō',\n",
       " 'ū',\n",
       " 'ȳ',\n",
       " '̄',\n",
       " '̈',\n",
       " '—',\n",
       " 'Ⓐ',\n",
       " 'Ⓑ',\n",
       " 'Ⓝ',\n",
       " 'Ⓟ',\n",
       " 'Ⓢ',\n",
       " 'ⓐ',\n",
       " 'ⓑ',\n",
       " 'ⓝ',\n",
       " 'ⓟ',\n",
       " 'ⓢ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset.charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilbert/distilgpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilbert/distilgpt2 and are newly initialized because the shapes did not match:\n",
      "- transformer.wte.weight: found shape torch.Size([50257, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "\t\"microsoft/swin-tiny-patch4-window7-224\",\n",
    "\t\"distilbert/distilgpt2\",\n",
    "\tdecoder_ignore_mismatched_sizes=True,\n",
    "\tdecoder_vocab_size=10,\n",
    "\tencoder_image_size=(2560, 1920),\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.size = [2560, 1920]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_padding(image_tensor, target_size, padding_value=0):\n",
    "\t\"\"\"\n",
    "\tResize a PyTorch tensor image while maintaining the aspect ratio by adding padding.\n",
    "\n",
    "\tArgs:\n",
    "\t\timage_tensor (torch.Tensor): The input image tensor of shape (C, H, W).\n",
    "\t\ttarget_size (tuple): The target size as (target_height, target_width).\n",
    "\t\tpadding_value (int): The value to use for padding.\n",
    "\n",
    "\tReturns:\n",
    "\t\ttorch.Tensor: The resized image tensor with padding.\n",
    "\t\"\"\"\n",
    "\t_, original_height, original_width = image_tensor.shape\n",
    "\ttarget_height, target_width = target_size\n",
    "\n",
    "\t# Calculate the scaling factor\n",
    "\tscale = min(target_width / original_width, target_height / original_height)\n",
    "\tnew_width = int(original_width * scale)\n",
    "\tnew_height = int(original_height * scale)\n",
    "\n",
    "\t# Resize the image\n",
    "\tresized_image = torch.nn.functional.interpolate(\n",
    "\t\timage_tensor.unsqueeze(0), size=(new_height, new_width), mode='bilinear', align_corners=False\n",
    "\t).squeeze(0)\n",
    "\n",
    "\t# Calculate padding\n",
    "\tpad_top = (target_height - new_height) // 2\n",
    "\tpad_bottom = target_height - new_height - pad_top\n",
    "\tpad_left = (target_width - new_width) // 2\n",
    "\tpad_right = target_width - new_width - pad_left\n",
    "\n",
    "\t# Apply padding\n",
    "\tpadded_image = torch.nn.functional.pad(\n",
    "\t\tresized_image, (pad_left, pad_right, pad_top, pad_bottom), value=padding_value\n",
    "\t)\n",
    "\n",
    "\treturn padded_image\n",
    "\n",
    "# Example usage\n",
    "target_size = (2560, 1920)\n",
    "resized_image = resize_with_padding(sample_data[\"imgs\"][0], target_size, padding_value=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2560, 1920])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x74095639de20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(resized_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(159.6405)\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(resized_image), torch.max(resized_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image = resized_image.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x740956312370>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sample = processor(resized_image, return_tensors=\"pt\")\n",
    "plt.imshow(np.transpose(input_sample[\"pixel_values\"][0], (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.image_processing_base.BatchFeature"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.0629, -0.0629, -0.0629,  ..., -0.0629, -0.0629, -0.0629],\n",
       "          [-0.0629, -0.0629, -0.0629,  ..., -0.0629, -0.0629, -0.0629],\n",
       "          [-0.0629, -0.0629, -0.0629,  ..., -0.0629, -0.0629, -0.0629],\n",
       "          ...,\n",
       "          [-0.0629, -0.0629, -0.0629,  ..., -0.0629, -0.0629, -0.0629],\n",
       "          [-0.0629, -0.0629, -0.0629,  ..., -0.0629, -0.0629, -0.0629],\n",
       "          [-0.0629, -0.0629, -0.0629,  ..., -0.0629, -0.0629, -0.0629]],\n",
       "\n",
       "         [[ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
       "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
       "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
       "          ...,\n",
       "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
       "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
       "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651]],\n",
       "\n",
       "         [[ 0.2871,  0.2871,  0.2871,  ...,  0.2871,  0.2871,  0.2871],\n",
       "          [ 0.2871,  0.2871,  0.2871,  ...,  0.2871,  0.2871,  0.2871],\n",
       "          [ 0.2871,  0.2871,  0.2871,  ...,  0.2871,  0.2871,  0.2871],\n",
       "          ...,\n",
       "          [ 0.2871,  0.2871,  0.2871,  ...,  0.2871,  0.2871,  0.2871],\n",
       "          [ 0.2871,  0.2871,  0.2871,  ...,  0.2871,  0.2871,  0.2871],\n",
       "          [ 0.2871,  0.2871,  0.2871,  ...,  0.2871,  0.2871,  0.2871]]]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.61 GiB of which 78.31 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.22 GiB is allocated by PyTorch, and 180.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_sample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:595\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[0;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 595\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    603\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\u001b[38;5;241m*\u001b[39mencoder_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:1062\u001b[0m, in \u001b[0;36mSwinModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m   1056\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdepths))\n\u001b[1;32m   1058\u001b[0m embedding_output, input_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1059\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[1;32m   1060\u001b[0m )\n\u001b[0;32m-> 1062\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1072\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:881\u001b[0m, in \u001b[0;36mSwinEncoder.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001b[0m\n\u001b[1;32m    872\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    873\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    874\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m         always_partition,\n\u001b[1;32m    879\u001b[0m     )\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 881\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    886\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:801\u001b[0m, in \u001b[0;36mSwinStage.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m    799\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    807\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:755\u001b[0m, in \u001b[0;36mSwinLayer.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    752\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m shortcut \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(attention_windows)\n\u001b[1;32m    754\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_after(hidden_states)\n\u001b[0;32m--> 755\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(layer_output)\n\u001b[1;32m    758\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m (layer_output, attention_outputs[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m (layer_output,)\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:621\u001b[0m, in \u001b[0;36mSwinIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    620\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 621\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DAN/lib/python3.9/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.61 GiB of which 78.31 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.22 GiB is allocated by PyTorch, and 180.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "output = model(**input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
